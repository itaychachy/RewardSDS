import os
import json
import argparse
from PIL import Image
import torch
import io
import numpy as np
from collections import defaultdict
from google import genai
from google.genai import types
import typing_extensions as typing
from typing import Union

# Define typed dictionaries for grading.
class Score(typing.TypedDict):
    score: float
    explanation: str


class Grading(typing.TypedDict):
    accuracy_to_prompt: Score
    creativity_and_originality: Score
    visual_quality_and_realism: Score
    consistency_and_cohesion: Score
    emotional_or_thematic_resonance: Score
    overall_score: Score


def get_verifier_prompt():
    """Return the verifier prompt for LLM-Grader."""
    return """You are a multimodal large-language model tasked with evaluating images
generated by a text-to-image model. Your goal is to assess each generated
image based on specific aspects and provide a detailed critique, along with
a scoring system. The final output should be formatted as a JSON object
containing individual scores for each aspect and an overall score. The keys
in the JSON object should be: `accuracy_to_prompt`, `creativity_and_originality`,
`visual_quality_and_realism`, `consistency_and_cohesion`,
`emotional_or_thematic_resonance`, and `overall_score`. Below is a comprehensive
guide to follow in your evaluation process:

1. Key Evaluation Aspects and Scoring Criteria:
For each aspect, provide a score from 0 to 10, where 0 represents poor
performance and 10 represents excellent performance. For each score, include
a short explanation or justification (1-2 sentences) explaining why that
score was given. The aspects to evaluate are as follows:

a) Accuracy to Prompt
Assess how well the image matches the description given in the prompt.
Consider whether all requested elements are present and if the scene,
objects, and setting align accurately with the text. Score: 0 (no
alignment) to 10 (perfect match to prompt).

b) Creativity and Originality
Evaluate the uniqueness and creativity of the generated image. Does the
model present an imaginative or aesthetically engaging interpretation of the
prompt? Is there any evidence of creativity beyond a literal interpretation?
Score: 0 (lacks creativity) to 10 (highly creative and original).

c) Visual Quality and Realism
Assess the overall visual quality, including resolution, detail, and realism.
Look for coherence in lighting, shading, and perspective. Even if the image
is stylized or abstract, judge whether the visual elements are well-rendered
and visually appealing. Score: 0 (poor quality) to 10 (high-quality and
realistic).

d) Consistency and Cohesion
Check for internal consistency within the image. Are all elements cohesive
and aligned with the prompt? For instance, does the perspective make sense,
and do objects fit naturally within the scene without visual anomalies?
Score: 0 (inconsistent) to 10 (fully cohesive and consistent).

e) Emotional or Thematic Resonance
Evaluate how well the image evokes the intended emotional or thematic tone of
the prompt. For example, if the prompt is meant to be serene, does the image
convey calmness? If it's adventurous, does it evoke excitement? Score: 0
(no resonance) to 10 (strong resonance with the prompt's theme).

2. Overall Score
After scoring each aspect individually, provide an overall score,
representing the model's general performance on this image. This should be
a weighted average based on the importance of each aspect to the prompt or an
average of all aspects.
"""


def load_prompts(json_path):
    """Load prompts from the given JSON file."""
    with open(json_path, "r") as f:
        data = json.load(f)
    # TODO: Expects a list, modify the data structure as needed based on the JSON file format.
    return data


def load_image(image_path):
    """Load an image from disk and convert it to RGB."""
    np_im = np.array(Image.open(image_path).convert("RGB"))
    return Image.fromarray(np_im.astype(np.uint8))


def convert_to_bytes(path_or_url: str) -> bytes:
    """Load an image from a path or URL and convert it to bytes (PNG format)."""
    image = load_image(path_or_url).convert("RGB")
    image_bytes_io = io.BytesIO()
    image.save(image_bytes_io, format="PNG")
    return image_bytes_io.getvalue()


def prepare_inputs(prompt: str, image_path: Union[str, bytes]):
    """Prepare inputs for the LLM-Grader API from a given prompt and image."""
    inputs = [
        types.Part.from_text(text=prompt),
        types.Part.from_bytes(data=convert_to_bytes(image_path), mime_type="image/png"),
    ]
    return inputs


def main():
    parser = argparse.ArgumentParser(
        description="Load generated images and prompts, and compute LLM-Grader scores."
    )
    # TODO: Modify the default values or set the required arguments based on your setup.
    parser.add_argument(
        "--data_path",
        type=str,
        default="data.json",
        help="Path to JSON file containing prompts."
    )
    parser.add_argument(
        "--images_dir",
        type=str,
        default="images/",
        help="Directory containing generated images."
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="llm_grader_scores.json",
        help="Output JSON file to save the LLM-Grader scores."
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Device to run the model on."
    )
    parser.add_argument(
        "--n_views",
        type=int,
        default=1,
        help="Number of views to evaluate for each prompt. Enables 3D evaluation, default value is 1 for 2D evaluation."
    )
    args = parser.parse_args()

    # TODO: Replace the empty string with your API key.
    # Set up the LLM-Grader client with the provided API key.
    api_key = ""
    client = genai.Client(api_key=api_key)

    # Prepare the system instruction (verifier prompt) inline.
    verifier_prompt = get_verifier_prompt()

    generation_config = types.GenerateContentConfig(
        system_instruction=verifier_prompt,
        response_mime_type="application/json",
        response_schema=list[Grading],
        seed=1994,
    )
    # Load prompts from the provided JSON file.
    prompts = load_prompts(args.data_path)
    results = defaultdict(list)
    for prompt in prompts:
        for i in range(args.n_views):
            # TODO: Modify this based on the path conversion in the generation script.
            # Convert prompt to a safe filename as in the generation script.
            safe_name = prompt.replace(" ", "_").replace("/", "_").replace(":", "_").replace(",", "_")
            image_path = os.path.join(args.images_dir, safe_name + f'{i}.png')
            if not os.path.exists(image_path):
                print(f"Warning: Image not found for prompt:\n  '{prompt}'\n  Expected at: {image_path}")
                continue

            inputs = prepare_inputs(prompt, image_path)
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=types.Content(parts=inputs, role="user"),
                config=generation_config
            )
            # Store the parsed response.
            results[prompt].append(response.parsed)
        print(json.dumps(response.parsed, indent=4))
        print("\n")

    # Save the LLM-Grader scores to the output JSON file.
    with open(args.output_path, "w") as f:
        json.dump(results, f, indent=4)
    print(f"LLM-Grader scores saved to {args.output_path}")

    # Calculate and print the average overall score if available.
    overall_scores = []
    for grading_list in results.values():
        s = 0.
        for g in grading_list:
            if g and isinstance(g, list):
                grading = g[0]
                overall = grading.get("overall_score", {}).get("score")
                if overall is not None:
                    s += overall
        s /= args.n_views
        overall_scores.append(s)
    if overall_scores:
        avg_score = sum(overall_scores) / len(overall_scores)
        print(f"Average Overall Score: {avg_score}")
    else:
        print("No overall scores computed.")


if __name__ == "__main__":
    main()
